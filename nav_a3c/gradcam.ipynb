{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCAM Visualization Demo with ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'StationAcademy' started successfully!\n",
      "Unity Academy name: StationAcademy\n",
      "        Reset Parameters : {}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from sys import stdout\n",
    "from collections import deque\n",
    "import copy\n",
    "from vis import vis_paths\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlagents.envs.environment import UnityEnvironment\n",
    "\n",
    "import imageio\n",
    "\n",
    "# set logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "env_name = '1226_final_without_visual_goals'\n",
    "path = \"C:/data/ml-agents-old/scripts/envs/{}/HBF-navigation-experiment.exe\".format(env_name)\n",
    "env = UnityEnvironment(file_name=path, worker_id=0, seed=1, no_graphics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import scipy.misc\n",
    "import scipy.signal\n",
    "import pickle\n",
    "\n",
    "import config\n",
    "import network\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, name, trainer, global_episode, model_path, env, brain_name):\n",
    "        self.name = name\n",
    "        self.trainer = trainer\n",
    "        self.global_episode = global_episode\n",
    "        self.summary_writer = tf.summary.FileWriter('./log/' + name)\n",
    "        self.network = network.Network(name, trainer) # local network\n",
    "        from_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "        to_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, name)\n",
    "        self.copy_network = [b.assign(a) for a, b in zip(from_var, to_var)] # op to sync from global network\n",
    "        self.model_path = model_path\n",
    "        self.brain_name = brain_name\n",
    "        self.env = env\n",
    "        self.env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "            \n",
    "    # static function to save frame during training\n",
    "    def make_gif(images, fname, duration=2, true_image=False, salience=False, salIMGS=None):\n",
    "        import moviepy.editor as mpy\n",
    "        def make_frame(t):\n",
    "            try:\n",
    "                x = images[int(len(images)/duration*t)]\n",
    "            except:\n",
    "                x = images[-1]\n",
    "            if true_image:\n",
    "                return x.astype(np.uint8)\n",
    "            else:\n",
    "                return ((x+1)/2*255).astype(np.uint8)\n",
    "        \n",
    "        def make_mask(t):\n",
    "            try:\n",
    "                x = salIMGS[int(len(salIMGS)/duration*t)]\n",
    "            except:\n",
    "                x = salIMGS[-1]\n",
    "            return x\n",
    "\n",
    "        clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "        if salience == True:\n",
    "            mask = mpy.VideoClip(make_mask, ismask=True,duration= duration)\n",
    "            clipB = clip.set_mask(mask)\n",
    "            clipB = clip.set_opacity(0)\n",
    "            mask = mask.set_opacity(0.1)\n",
    "            mask.write_gif(fname, fps = len(images) / duration,verbose=False)\n",
    "        else:\n",
    "            clip.write_gif(fname, fps = len(images) / duration,verbose=False)\n",
    "\n",
    "    # static function\n",
    "    def resize_image(image):\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        return image\n",
    "        #return scipy.misc.imresize(image, [84, 84])\n",
    "        \n",
    "    # !!!!!\n",
    "    # stack sliding windows into discret 1d array\n",
    "    def window_stack(self, a, stepsize=10, trim=2):\n",
    "        a = a[trim:a.shape[0]-trim, trim:a.shape[1]-trim]\n",
    "        b = []\n",
    "        for i in range(int(a.shape[0] / stepsize)):\n",
    "            for j in range(int(a.shape[1] / stepsize)):\n",
    "                b.append(np.mean(a[i*stepsize:i*stepsize+stepsize,j*stepsize:j*stepsize+stepsize]))\n",
    "\n",
    "        b = np.array(b)\n",
    "        # map to [0, 8) int\n",
    "        b = b * 7\n",
    "        b = b.astype(int)\n",
    "\n",
    "        return b\n",
    "\n",
    "    def discount(x, gamma):\n",
    "        return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]        \n",
    "\n",
    "    def train(self, train_buffer, sess, boot_value):\n",
    "        train_buffer = np.array(train_buffer)\n",
    "        # unroll from train_buffer\n",
    "        input_image = np.array(train_buffer[:, 0].tolist())\n",
    "        aux_action = np.array(train_buffer[:, 1].tolist())\n",
    "        aux_reward = np.array(train_buffer[:, 2:3].tolist())\n",
    "        aux_velocity = np.array(train_buffer[:, 3].tolist())\n",
    "        action = train_buffer[:, 4]\n",
    "        reward = train_buffer[:, 5]\n",
    "        value = train_buffer[:, 6]\n",
    "        depth_pred = train_buffer[:, 7] # <- ?\n",
    "        true_depth = np.array(train_buffer[:, 8].tolist())\n",
    "\n",
    "        reward_plus = np.asarray(reward.tolist() + [boot_value])\n",
    "        disc_reward = Agent.discount(reward_plus, config.GAMMA)[:-1]\n",
    "        value_plus = np.asarray(value.tolist())\n",
    "        #advantage = Agent.discount(reward + config.GAMMA*value_plus[1:] - value_plus[:-1], config.GAMMA)\n",
    "        advantage = disc_reward - value_plus\n",
    "        vl, pl, el, dl, dl2, gradn, _ , d_tmp= sess.run([self.network.value_loss,\n",
    "            self.network.policy_loss,\n",
    "            self.network.entropy_loss,\n",
    "            self.network.depth_loss,\n",
    "            self.network.depth_loss2,\n",
    "            self.network.gradient_norm,\n",
    "            self.network.apply_gradient, self.network.depth_loss_], feed_dict={\n",
    "                self.network.input_image: input_image,\n",
    "                self.network.input_action: aux_action,\n",
    "                self.network.input_reward: aux_reward,\n",
    "                self.network.input_velocity: aux_velocity,\n",
    "                self.network.true_value: disc_reward,\n",
    "                self.network.advantage: advantage,\n",
    "                self.network.action: action,\n",
    "                self.network.true_depth: true_depth,\n",
    "                self.network.lstm1_state_c_in: self.train_lstm1_state_c,\n",
    "                self.network.lstm1_state_h_in: self.train_lstm1_state_h,\n",
    "                self.network.lstm2_state_c_in: self.train_lstm2_state_c,\n",
    "                self.network.lstm2_state_h_in: self.train_lstm2_state_h\n",
    "            })\n",
    "        sys.stdout.flush()\n",
    "        return vl, pl, el, dl, dl2, gradn, _\n",
    "\n",
    "    def get_action(self, action):\n",
    "        move_forward  = action // 3\n",
    "        rotate = action % 3\n",
    "        return [move_forward, rotate]\n",
    "\n",
    "    def get_vel(self, prev_coord, coord, action_transformed):\n",
    "        if action_transformed[0] == 0:\n",
    "            distance = np.linalg.norm(coord-prev_coord)\n",
    "            # cos 15 degrees, corresponding to turning angle in unity\n",
    "            if action_transformed[1] == 0:\n",
    "                result = [0.96*distance, (coord[1]-prev_coord[1]), -0.26*distance, 0, -15, 0]\n",
    "            if action_transformed[1] == 1:\n",
    "                # cos 15 degrees, corresponding to turning angle in unity\n",
    "                result = [distance, (coord[1]-prev_coord[1]), 0, 0, 0, 0]\n",
    "            if action_transformed[1] == 2:\n",
    "                result = [0.96*distance, (coord[1]-prev_coord[1]), 0.26*distance, 0, 15, 0]\n",
    "\n",
    "        elif action_transformed[0] == 1:\n",
    "            if action_transformed[1] == 0:\n",
    "                result = [0, 0, 0, 0, -15, 0]\n",
    "            if action_transformed[1] == 1:\n",
    "                # cos 15 degrees, corresponding to turning angle in unity\n",
    "                result = [0, 0, 0, 0, 0, 0]\n",
    "            if action_transformed[1] == 2:\n",
    "                result = [0, 0, 0, 0, 15, 0]\n",
    "        else:\n",
    "            assert 0, \"action_transformed's first element is unknown\"\n",
    "\n",
    "        result = np.array(result)\n",
    "        return result\n",
    "\n",
    "    def run(self, sess, trainer, saver, coordinator):\n",
    "        print('starting agent:', self.name)\n",
    "        sys.stdout.flush()\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coordinator.should_stop():\n",
    "                sess.run(self.global_episode.assign_add(1))\n",
    "                print('episode:', sess.run(self.global_episode))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                ep = sess.run(self.global_episode)\n",
    "                ep_reward = 0\n",
    "                ep_step = 0\n",
    "                ep_start_time = time.time()\n",
    "\n",
    "                sess.run(self.copy_network)\n",
    "                self.train_buffer = []\n",
    "                frame_buffer = []\n",
    "                running = True\n",
    "\n",
    "                # !!!!!!\n",
    "                # self.game.reset()\n",
    "                # rgb, prev_d = self.game.frame()\n",
    "                rgb = np.asarray(self.env_info.visual_observations[1][0])\n",
    "                prev_d = self.window_stack(np.asarray(self.env_info.visual_observations[0][0]))\n",
    "\n",
    "                frame_buffer.append(rgb * 255)\n",
    "                # rgb = Agent.resize_image(rgb)\n",
    "                prev_act_idx = 0\n",
    "                prev_reward = 0\n",
    "                prev_vel = np.array([0.0]*6)\n",
    "                prev_coord = self.env_info.vector_observations[0][-3:]\n",
    "\n",
    "                self.lstm1_state_c, self.lstm1_state_h, self.lstm2_state_c, self.lstm2_state_h = self.network.lstm1_init_state_c, self.network.lstm1_init_state_h,self.network.lstm2_init_state_c,self.network.lstm2_init_state_h\n",
    "                \n",
    "                self.env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "                print('initial position:({:.2f} {:.2f} {:.2f})'.format(self.env_info.vector_observations[0][-3:][0], \n",
    "                                                                       self.env_info.vector_observations[0][-3:][1],\n",
    "                        self.env_info.vector_observations[0][-3:][2]))\n",
    "                # !!!!!!\n",
    "                # while self.game.running():\n",
    "                ep_step_max = 1000\n",
    "                positions = []\n",
    "                while True:\n",
    "                    if len(self.train_buffer)==0:\n",
    "                        self.train_lstm1_state_h = self.lstm1_state_h\n",
    "                        self.train_lstm1_state_c = self.lstm1_state_c\n",
    "                        self.train_lstm2_state_h = self.lstm2_state_h\n",
    "                        self.train_lstm2_state_c = self.lstm2_state_c\n",
    "                    act_prob, pred_value, depth_pred, self.lstm1_state_c, self.lstm1_state_h, self.lstm2_state_c, self.lstm2_state_h = sess.run([self.network.policy,\n",
    "                        self.network.value, self.network.depth_pred,\n",
    "                        self.network.lstm1_state_c_out, \n",
    "                        self.network.lstm1_state_h_out, \n",
    "                        self.network.lstm2_state_c_out, \n",
    "                        self.network.lstm2_state_h_out]\n",
    "                        , \n",
    "                        feed_dict={self.network.input_image: [rgb], \n",
    "                        self.network.input_action: [prev_act_idx], \n",
    "                        self.network.input_reward: [[prev_reward]], \n",
    "                        self.network.input_velocity: [prev_vel],\n",
    "                        self.network.lstm1_state_c_in:self.lstm1_state_c,\n",
    "                        self.network.lstm1_state_h_in:self.lstm1_state_h,\n",
    "                        self.network.lstm2_state_c_in:self.lstm2_state_c,\n",
    "                        self.network.lstm2_state_h_in:self.lstm2_state_h\n",
    "                    })\n",
    "                    \n",
    "                    self.action_visualize = act_prob\n",
    "                    action = np.random.choice(act_prob[0], p=act_prob[0])\n",
    "                    action_idx = np.argmax(act_prob==action)\n",
    "                    \n",
    "                    # !!!!!!\n",
    "                    # rgb_next, d, vel, reward, running = self.game.step(action_idx)\n",
    "                    action_transformed = self.get_action(action_idx)\n",
    "                    self.env_info = self.env.step(action_transformed)[self.brain_name] # send the action to the environment\n",
    "                    rgb_next = np.asarray(self.env_info.visual_observations[1][0])  # get the next state\n",
    "                    d = self.window_stack(np.asarray(self.env_info.visual_observations[0][0]))\n",
    "\n",
    "                    reward = self.env_info.rewards[0]                   # get the reward\n",
    "                    done = self.env_info.local_done[0]                  # see if episode has finished\n",
    "                    coord = self.env_info.vector_observations[0][-3:]\n",
    "\n",
    "                    # !!!!!!\n",
    "                    # only an estimation, get velocity from previous and current coordination, and action vector\n",
    "                    vel = self.get_vel(prev_coord, coord, action_transformed)\n",
    "                    \n",
    "                    sys.stdout.write('\\r episode:{}, step: {}, position:({:.2f} {:.2f} {:.2f}), score:{:.2f}, action:{}'.format(self.name, ep_step,\n",
    "                        self.env_info.vector_observations[0][-3:][0], self.env_info.vector_observations[0][-3:][1],\n",
    "                        self.env_info.vector_observations[0][-3:][2], ep_reward, str(action_idx)))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                    positions.append([self.env_info.vector_observations[0][-3:][0], \n",
    "                                     self.env_info.vector_observations[0][-3:][1], \n",
    "                                     self.env_info.vector_observations[0][-3:][2]])\n",
    "                    \n",
    "                    self.train_buffer.append([rgb, prev_act_idx, prev_reward, prev_vel, action_idx, \n",
    "                                         reward, pred_value[0][0], depth_pred, prev_d])\n",
    "\n",
    "                    ep_reward += reward\n",
    "                    ep_step += 1\n",
    "                    \n",
    "                    running = not ((ep_step >= ep_step_max) or done)\n",
    "\n",
    "                    if running:\n",
    "                        if ep%config.SAVE_PERIOD==0:\n",
    "                            frame_buffer.append(rgb_next * 255)\n",
    "                        # rgb_next = Agent.resize_image(rgb_next)\n",
    "                        rgb = rgb_next\n",
    "                    \n",
    "                    prev_act_idx = action_idx\n",
    "                    prev_reward = reward\n",
    "                    prev_vel = vel\n",
    "                    prev_d = d\n",
    "\n",
    "                    if len(self.train_buffer)==config.GRADIENT_CHUNK and running:\n",
    "                        boot_value = sess.run(self.network.value, feed_dict={\n",
    "                            self.network.input_image: [rgb], \n",
    "                            self.network.input_action: [prev_act_idx], \n",
    "                            self.network.input_reward: [[prev_reward]], \n",
    "                            self.network.input_velocity: [prev_vel],\n",
    "                            self.network.lstm1_state_c_in:self.lstm1_state_c,\n",
    "                            self.network.lstm1_state_h_in:self.lstm1_state_h,\n",
    "                            self.network.lstm2_state_c_in:self.lstm2_state_c,\n",
    "                            self.network.lstm2_state_h_in:self.lstm2_state_h\n",
    "                        })\n",
    "                        vl, pl, el, dl, dl2, gradn, _ = self.train(self.train_buffer, sess, boot_value)\n",
    "                        self.train_buffer = []\n",
    "                        sess.run(self.copy_network)\n",
    "                    if not running:\n",
    "                        break\n",
    "                if len(self.train_buffer)>0:\n",
    "                    vl, pl, el, dl, dl2, gradn, _ = self.train(self.train_buffer, sess, 0.0)\n",
    "                    self.test1 = [vl, pl, el, dl, dl2, gradn]\n",
    "\n",
    "                ep_finish_time = time.time()\n",
    "                print(self.name, 'elapse', str(int(ep_finish_time-ep_start_time)), 'seconds, reward:',ep_reward)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                \n",
    "                if ep%config.SAVE_PERIOD==0:\n",
    "                    imgs = np.array(frame_buffer)\n",
    "                    Agent.make_gif(imgs, './frame/image'+str(ep)+'_'+str(ep_reward)+'.gif', duration=len(imgs)*0.066, true_image=True, salience=False)\n",
    "                    print('frame saved')\n",
    "                    sys.stdout.flush()\n",
    "                \n",
    "\n",
    "                if ep%config.SAVE_PERIOD==0:\n",
    "                    saver.save(sess, self.model_path+'/model'+str(ep)+'.cptk')\n",
    "                    print('model saved')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(pl))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(vl))\n",
    "                    summary.value.add(tag='Losses/Entropy Loss', simple_value=float(el))\n",
    "                    summary.value.add(tag='Losses/Depth Loss', simple_value=float(dl))\n",
    "                    summary.value.add(tag='Losses/Depth Loss2', simple_value=float(dl2))\n",
    "                    summary.value.add(tag='Losses/Gradient Norm', simple_value=float(gradn))\n",
    "                    summary.value.add(tag='Performance/Reward', simple_value=float(ep_reward))\n",
    "                    self.summary_writer.add_summary(summary, ep)\n",
    "                    self.summary_writer.flush()\n",
    "                    \n",
    "                if ep%config.SAVE_PERIOD==0:\n",
    "                    print('save positions')\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                    with open('./positions/'+str(ep), 'wb') as fp:\n",
    "                        pickle.dump(positions, fp)\n",
    "                        \n",
    "                        \n",
    "\n",
    "    def evaluate(self, sess, saver, coordinator):\n",
    "        print('evaluation:', self.name)\n",
    "        sys.stdout.flush()\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coordinator.should_stop():\n",
    "                sess.run(self.global_episode.assign_add(1))\n",
    "                print('episode:', sess.run(self.global_episode))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                ep = sess.run(self.global_episode)\n",
    "                ep_reward = 0\n",
    "                ep_step = 0\n",
    "\n",
    "                sess.run(self.copy_network)\n",
    "                frame_buffer = []\n",
    "                running = True\n",
    "\n",
    "                rgb = np.asarray(self.env_info.visual_observations[1][0])\n",
    "\n",
    "                frame_buffer.append(rgb * 255)\n",
    "                \n",
    "                self.lstm1_state_c, self.lstm1_state_h, self.lstm2_state_c, self.lstm2_state_h = self.network.lstm1_init_state_c, self.network.lstm1_init_state_h,self.network.lstm2_init_state_c,self.network.lstm2_init_state_h\n",
    "                \n",
    "                self.env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "                print('initial position:({:.2f} {:.2f} {:.2f})'.format(self.env_info.vector_observations[0][-3:][0], \n",
    "                                                                       self.env_info.vector_observations[0][-3:][1],\n",
    "                                                                       self.env_info.vector_observations[0][-3:][2]))\n",
    "\n",
    "                ep_step_max = 1000\n",
    "                positions = []\n",
    "                while True:\n",
    "                    \n",
    "                    self.train_lstm1_state_h = self.lstm1_state_h\n",
    "                    self.train_lstm1_state_c = self.lstm1_state_c\n",
    "                    self.train_lstm2_state_h = self.lstm2_state_h\n",
    "                    self.train_lstm2_state_c = self.lstm2_state_c\n",
    "                    act_prob, pred_value, depth_pred, self.lstm1_state_c, self.lstm1_state_h, self.lstm2_state_c, self.lstm2_state_h = sess.run([self.network.policy,\n",
    "                        self.network.value, self.network.depth_pred,\n",
    "                        self.network.lstm1_state_c_out, \n",
    "                        self.network.lstm1_state_h_out, \n",
    "                        self.network.lstm2_state_c_out, \n",
    "                        self.network.lstm2_state_h_out], \n",
    "                        feed_dict={self.network.input_image: [rgb], \n",
    "                        self.network.input_action: [prev_act_idx], \n",
    "                        self.network.input_reward: [[prev_reward]], \n",
    "                        self.network.input_velocity: [prev_vel],\n",
    "                        self.network.lstm1_state_c_in:self.lstm1_state_c,\n",
    "                        self.network.lstm1_state_h_in:self.lstm1_state_h,\n",
    "                        self.network.lstm2_state_c_in:self.lstm2_state_c,\n",
    "                        self.network.lstm2_state_h_in:self.lstm2_state_h})\n",
    "                    \n",
    "                    self.action_visualize = act_prob\n",
    "                    action = np.random.choice(act_prob[0], p=act_prob[0])\n",
    "                    action_idx = np.argmax(act_prob==action)\n",
    "                    \n",
    "                    action_transformed = self.get_action(action_idx)\n",
    "                    self.env_info = self.env.step(action_transformed)[self.brain_name] # send the action to the environment\n",
    "                    rgb_next = np.asarray(self.env_info.visual_observations[1][0])  # get the next state\n",
    "                    d = self.window_stack(np.asarray(self.env_info.visual_observations[0][0]))\n",
    "\n",
    "                    reward = self.env_info.rewards[0]                   # get the reward\n",
    "                    done = self.env_info.local_done[0]                  # see if episode has finished\n",
    "                    coord = self.env_info.vector_observations[0][-3:]\n",
    "\n",
    "                    vel = self.get_vel(prev_coord, coord, action_transformed)\n",
    "                    \n",
    "                    sys.stdout.write('\\r episode:{}, step: {}, position:({:.2f} {:.2f} {:.2f}), score:{:.2f}, action:{}'.format(self.name, ep_step,\n",
    "                        self.env_info.vector_observations[0][-3:][0], self.env_info.vector_observations[0][-3:][1],\n",
    "                        self.env_info.vector_observations[0][-3:][2], ep_reward, str(action_idx)))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                    positions.append([self.env_info.vector_observations[0][-3:][0], \n",
    "                                     self.env_info.vector_observations[0][-3:][1], \n",
    "                                     self.env_info.vector_observations[0][-3:][2]])\n",
    "                    \n",
    "                    ep_reward += reward\n",
    "                    ep_step += 1\n",
    "                    \n",
    "                    running = not ((ep_step >= ep_step_max) or done)\n",
    "\n",
    "                    if running:\n",
    "                        if ep%config.SAVE_PERIOD==0:\n",
    "                            frame_buffer.append(rgb_next * 255)\n",
    "                        # rgb_next = Agent.resize_image(rgb_next)\n",
    "                        rgb = rgb_next\n",
    "                    \n",
    "                    if not running:\n",
    "                        break\n",
    "                        \n",
    "#                 if ep%config.SAVE_PERIOD==0:\n",
    "#                     imgs = np.array(frame_buffer)\n",
    "#                     Agent.make_gif(imgs, './frame/image'+str(ep)+'_'+str(ep_reward)+'.gif', duration=len(imgs)*0.066, true_image=True, salience=False)\n",
    "#                     print('frame saved')\n",
    "#                     sys.stdout.flush()\n",
    "                                    \n",
    "#                 if ep%config.SAVE_PERIOD==0:\n",
    "#                     print('save positions')\n",
    "#                     sys.stdout.flush()\n",
    "                    \n",
    "#                     with open('./positions/'+str(ep), 'wb') as fp:\n",
    "#                         pickle.dump(positions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:Connected new brain:\n",
      "Unity brain name: hbf-agent\n",
      "        Number of Visual Observations (per agent): 2\n",
      "        Camera Resolutions: ['CameraResolution(84, 84, 1)', 'CameraResolution(84, 84, 3)']\n",
      "        Vector Observation space size (per agent): 9\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [2, 3]\n",
      "        Vector Action descriptions: \n"
     ]
    }
   ],
   "source": [
    "env.step()\n",
    "default_brain = env.external_brain_names[0]\n",
    "brain = env.brains[default_brain]\n",
    "env_info = env.reset(train_mode=True)[default_brain]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt.model_checkpoint_path\n",
    "# from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "# print_tensors_in_checkpoint_file(file_name=\"./model/1226_final_without_visual_goals\\\\model5020.cptk\", tensor_name='', all_tensors=False)\n",
    "\n",
    "evaluation = True\n",
    "model_path = \"./model/1226_final_without_visual_goals\"\n",
    "model_time = \"1577341381.6629117\"\n",
    "\n",
    "# model_path = './model'\n",
    "frame_path = './frame'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if not os.path.exists(frame_path):\n",
    "    os.makedirs(frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:8: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:9: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:11: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:30: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:39: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:105: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:124: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\Desktop\\scl-navigation\\nav_a3c\\network.py:124: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\ztwang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import threading\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import config\n",
    "import network\n",
    "# import agent\n",
    "    \n",
    "with tf.device('cpu:0'):\n",
    "    global_episode = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
    "    trainer = tf.train.RMSPropOptimizer(config.LEARNING_RATE, decay=config.DECAY, \n",
    "                                        momentum=config.MOMENTUM, epsilon=config.EPSILON)\n",
    "    master_network = network.Network('global', trainer)\n",
    "    agent = Agent('thread_0_'+model_time, trainer, global_episode, \n",
    "                           model_path, env, default_brain)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/1226_final_without_visual_goals\\model5040.cptk\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace vanila relu to guided relu to get guided backpropagation.\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "\n",
    "@ops.RegisterGradient(\"GuidedRelu\")\n",
    "def _GuidedReluGrad(op, grad):\n",
    "    return tf.where(0. < grad, gen_nn_ops._relu_grad(grad, op.outputs[0]), tf.zeros_like(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'thread_0_1577341381.6629117/Variable:0' shape=(8, 8, 3, 16) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_1:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_2:0' shape=(4, 4, 16, 32) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_4:0' shape=(3872, 256) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_5:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/lstm1/rnn/basic_lstm_cell/kernel:0' shape=(321, 256) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/lstm1/rnn/basic_lstm_cell/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/lstm2/rnn/basic_lstm_cell/kernel:0' shape=(4204, 1024) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/lstm2/rnn/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_6:0' shape=(256, 6) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_7:0' shape=(6,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_8:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_9:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_10:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_11:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_12:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_13:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_14:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_15:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_16:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/Variable_17:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable/RMSProp:0' shape=(8, 8, 3, 16) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable/RMSProp_1:0' shape=(8, 8, 3, 16) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_1/RMSProp:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_1/RMSProp_1:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_2/RMSProp:0' shape=(4, 4, 16, 32) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_2/RMSProp_1:0' shape=(4, 4, 16, 32) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_3/RMSProp:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_3/RMSProp_1:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_4/RMSProp:0' shape=(3872, 256) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_4/RMSProp_1:0' shape=(3872, 256) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_5/RMSProp:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_5/RMSProp_1:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm1/rnn/basic_lstm_cell/kernel/RMSProp:0' shape=(321, 256) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm1/rnn/basic_lstm_cell/kernel/RMSProp_1:0' shape=(321, 256) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm1/rnn/basic_lstm_cell/bias/RMSProp:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm1/rnn/basic_lstm_cell/bias/RMSProp_1:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm2/rnn/basic_lstm_cell/kernel/RMSProp:0' shape=(4204, 1024) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm2/rnn/basic_lstm_cell/kernel/RMSProp_1:0' shape=(4204, 1024) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm2/rnn/basic_lstm_cell/bias/RMSProp:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/lstm2/rnn/basic_lstm_cell/bias/RMSProp_1:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_6/RMSProp:0' shape=(256, 6) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_6/RMSProp_1:0' shape=(256, 6) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_7/RMSProp:0' shape=(6,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_7/RMSProp_1:0' shape=(6,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_8/RMSProp:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_8/RMSProp_1:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_9/RMSProp:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_9/RMSProp_1:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_10/RMSProp:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_10/RMSProp_1:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_11/RMSProp:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_11/RMSProp_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_12/RMSProp:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_12/RMSProp_1:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_13/RMSProp:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_13/RMSProp_1:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_14/RMSProp:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_14/RMSProp_1:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_15/RMSProp:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_15/RMSProp_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_16/RMSProp:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_16/RMSProp_1:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_17/RMSProp:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'thread_0_1577341381.6629117/global/Variable_17/RMSProp_1:0' shape=(512,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='thread_0_'+model_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"thread_0_1577341381.6629117\", reuse=True): # root variable scope\n",
    "#     bar3 = tf.get_variable('global/Variable_17/RMSProp_1:0') # reuse (equivalent to the above)\n",
    "bar2 = [var for var in tf.global_variables() if var.op.name==\"thread_0_1577341381.6629117/global/Variable_17/RMSProp_1\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'thread_0_1577341381.6629117/global/Variable_17/RMSProp_1:0' shape=(512,) dtype=float32_ref>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/1226_final_without_visual_goals\\model5040.cptk\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "slim = tf.contrib.slim\n",
    "import gradcam_utils as utils\n",
    "eval_graph = tf.Graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    with eval_graph.as_default():\n",
    "        with eval_graph.gradient_override_map({'Relu': 'GuidedRelu'}):\n",
    "            \n",
    "            images = tf.placeholder(\"float\", [224, 224, 3])\n",
    "            labels = tf.placeholder(tf.float32, [6])\n",
    "            frame_buffer = []\n",
    "            agent.network\n",
    "\n",
    "            sess.run(agent.copy_network)\n",
    "            ep_reward = 0\n",
    "            ep_step = 0\n",
    "\n",
    "            running = True\n",
    "\n",
    "            rgb = np.asarray(agent.env_info.visual_observations[1][0])\n",
    "\n",
    "            agent.train_buffer = []\n",
    "\n",
    "            prev_d = agent.window_stack(np.asarray(agent.env_info.visual_observations[0][0]))\n",
    "\n",
    "            frame_buffer.append(rgb * 255)\n",
    "            # rgb = Agent.resize_image(rgb)\n",
    "            prev_act_idx = 0\n",
    "            prev_reward = 0\n",
    "            prev_vel = np.array([0.0]*6)\n",
    "            prev_coord = agent.env_info.vector_observations[0][-3:]\n",
    "\n",
    "            agent.lstm1_state_c, agent.lstm1_state_h, agent.lstm2_state_c, agent.lstm2_state_h = agent.network.lstm1_init_state_c, agent.network.lstm1_init_state_h, agent.network.lstm2_init_state_c, agent.network.lstm2_init_state_h\n",
    "            agent.env_info = agent.env.reset(train_mode=True)[agent.brain_name]\n",
    "            print('initial position:({:.2f} {:.2f} {:.2f})'.format(env_info.vector_observations[0][-3:][0], \n",
    "                                                                   env_info.vector_observations[0][-3:][1],\n",
    "                                                                   env_info.vector_observations[0][-3:][2]))\n",
    "\n",
    "            ep_step_max = 1000\n",
    "            positions = []\n",
    "            \n",
    "            act_prob, pred_value, depth_pred, lstm1_state_c, lstm1_state_h, lstm2_state_c, lstm2_state_h = sess.run([agent.network.policy,\n",
    "                agent.network.value, agent.network.depth_pred,\n",
    "                agent.network.lstm1_state_c_out, \n",
    "                agent.network.lstm1_state_h_out, \n",
    "                agent.network.lstm2_state_c_out, \n",
    "                agent.network.lstm2_state_h_out], \n",
    "                feed_dict = {agent.network.input_image: [rgb], \n",
    "                agent.network.input_action: [prev_act_idx], \n",
    "                agent.network.input_reward: [[prev_reward]], \n",
    "                agent.network.input_velocity: [prev_vel],\n",
    "                agent.network.lstm1_state_c_in:agent.lstm1_state_c,\n",
    "                agent.network.lstm1_state_h_in:agent.lstm1_state_h,\n",
    "                agent.network.lstm2_state_c_in:agent.lstm2_state_c,\n",
    "                agent.network.lstm2_state_h_in:agent.lstm2_state_h})\n",
    "\n",
    "            agent.action_visualize = act_prob\n",
    "            action = np.random.choice(act_prob[0], p=act_prob[0])\n",
    "            action_idx = np.argmax(act_prob==action)\n",
    "\n",
    "            # !!!!!!!\n",
    "            cost = (-1) * tf.reduce_sum(tf.multiply(labels, tf.log(act_prob)), axis=1)\n",
    "            net = tf.log(act_prob)\n",
    "            print(\"cost\", cost)\n",
    "            print(\"net\", net)\n",
    "\n",
    "            # !!!!!!!\n",
    "            with tf.variable_scope(\"thread_0_1577341381.6629117\", reuse=tf.AUTO_REUSE): # root variable scope\n",
    "                bar3 = tf.get_variable('global/Variable_17/RMSProp_1:0') # reuse (equivalent to the above)\n",
    "\n",
    "            target_conv_layer = [var for var in tf.global_variables() if var.op.name==\"thread_0_1577341381.6629117/global/Variable_17/RMSProp_1\"]\n",
    "        \n",
    "            # with tf.variable_scope('thread_'+str(i)+'_'+model_time, reuse=True):\n",
    "            #     v1 = tf.get_variable(\"var\", [1])\n",
    "\n",
    "            y_c = tf.reduce_sum(tf.multiply(net, labels), axis=1)\n",
    "            print('y_c:', y_c)\n",
    "            target_conv_layer_grad = tf.gradients(y_c, target_conv_layer)\n",
    "            print('target_conv_layer_grad:', target_conv_layer_grad)\n",
    "\n",
    "            # Guided backpropagtion back to input layer\n",
    "            gb_grad = tf.gradients(cost, rgb)[0]\n",
    "            print('gb_grad:', gb_grad)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'thread_0_1577341381.6629117/global/Variable_17/RMSProp_1:0' shape=(512,) dtype=float32_ref>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if var.op.name==\"thread_0_1577341381.6629117/Variable_5:0\"\n",
    "bar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"nav_a3c/model/1226_final_without_visual_goals/\"\n",
    "model.load_weights(checkpoint_path)\n",
    "ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='lstm1'))\n",
    "print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name))\n",
    "# agent_arr[0].network.policy\n",
    "# current_scope = tf.contrib.framework.get_name_scope()\n",
    "# print(current_scope)\n",
    "# tf.get_variable('thread_0_1577341381.6629117/Variable_5', [], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session(graph=eval_graph) as sess:    \n",
    "    sess.run(init)\n",
    "    saver.restore(sess, latest_checkpoint)    \n",
    "            \n",
    "            images = tf.placeholder(\"float\", [224, 224, 3])\n",
    "            labels = tf.placeholder(tf.float32, [6])\n",
    "            frame_buffer = []\n",
    "            agent.network\n",
    "\n",
    "            sess.run(agent.copy_network)\n",
    "            ep_reward = 0\n",
    "            ep_step = 0\n",
    "\n",
    "            running = True\n",
    "\n",
    "            rgb = np.asarray(agent.env_info.visual_observations[1][0])\n",
    "\n",
    "            agent.train_buffer = []\n",
    "\n",
    "            prev_d = agent.window_stack(np.asarray(agent.env_info.visual_observations[0][0]))\n",
    "\n",
    "            frame_buffer.append(rgb * 255)\n",
    "            # rgb = Agent.resize_image(rgb)\n",
    "            prev_act_idx = 0\n",
    "            prev_reward = 0\n",
    "            prev_vel = np.array([0.0]*6)\n",
    "            prev_coord = agent.env_info.vector_observations[0][-3:]\n",
    "\n",
    "            agent.lstm1_state_c, agent.lstm1_state_h, agent.lstm2_state_c, agent.lstm2_state_h = agent.network.lstm1_init_state_c, agent.network.lstm1_init_state_h, agent.network.lstm2_init_state_c, agent.network.lstm2_init_state_h\n",
    "            agent.env_info = agent.env.reset(train_mode=True)[agent.brain_name]\n",
    "            print('initial position:({:.2f} {:.2f} {:.2f})'.format(env_info.vector_observations[0][-3:][0], \n",
    "                                                                   env_info.vector_observations[0][-3:][1],\n",
    "                                                                   env_info.vector_observations[0][-3:][2]))\n",
    "\n",
    "            ep_step_max = 1000\n",
    "            positions = []\n",
    "            while True:\n",
    "                act_prob, pred_value, depth_pred, lstm1_state_c, lstm1_state_h, lstm2_state_c, lstm2_state_h = sess.run([agent.network.policy,\n",
    "                    agent.network.value, agent.network.depth_pred,\n",
    "                    agent.network.lstm1_state_c_out, \n",
    "                    agent.network.lstm1_state_h_out, \n",
    "                    agent.network.lstm2_state_c_out, \n",
    "                    agent.network.lstm2_state_h_out], \n",
    "                    feed_dict = {agent.network.input_image: [rgb], \n",
    "                    agent.network.input_action: [prev_act_idx], \n",
    "                    agent.network.input_reward: [[prev_reward]], \n",
    "                    agent.network.input_velocity: [prev_vel],\n",
    "                    agent.network.lstm1_state_c_in:agent.lstm1_state_c,\n",
    "                    agent.network.lstm1_state_h_in:agent.lstm1_state_h,\n",
    "                    agent.network.lstm2_state_c_in:agent.lstm2_state_c,\n",
    "                    agent.network.lstm2_state_h_in:agent.lstm2_state_h})\n",
    "\n",
    "                agent.action_visualize = act_prob\n",
    "                action = np.random.choice(act_prob[0], p=act_prob[0])\n",
    "                action_idx = np.argmax(act_prob==action)\n",
    "                \n",
    "                # !!!!!!!\n",
    "                cost = (-1) * tf.reduce_sum(tf.multiply(labels, tf.log(act_prob)), axis=1)\n",
    "                \n",
    "                net = tf.log(act_prob)\n",
    "                \n",
    "                # !!!!!!!\n",
    "                target_conv_layer = [var for var in tf.global_variables() if var.op.name==\"thread_0_1577341381.6629117/global/Variable_5/RMSProp_1\"]\n",
    "                # with tf.variable_scope('thread_'+str(i)+'_'+model_time, reuse=True):\n",
    "                #     v1 = tf.get_variable(\"var\", [1])\n",
    "                \n",
    "                y_c = tf.reduce_sum(tf.multiply(net, labels), axis=1)\n",
    "                print('y_c:', y_c)\n",
    "                target_conv_layer_grad = tf.gradients(y_c, target_conv_layer)\n",
    "                print('target_conv_layer_grad:', target_conv_layer_grad)\n",
    "\n",
    "                # Guided backpropagtion back to input layer\n",
    "                gb_grad = tf.gradients(cost, rgb)[0]\n",
    "                print('gb_grad:', gb_grad)\n",
    "\n",
    "                init = tf.global_variables_initializer()\n",
    "\n",
    "                action_transformed = agent.get_action(action_idx)\n",
    "                agent.env_info = agent.env.step(action_transformed)[agent.brain_name] # send the action to the environment\n",
    "                rgb_next = np.asarray(agent.env_info.visual_observations[1][0])  # get the next state\n",
    "                d = agent.window_stack(np.asarray(agent.env_info.visual_observations[0][0]))\n",
    "\n",
    "                reward = agent.env_info.rewards[0]                   # get the reward\n",
    "                done = agent.env_info.local_done[0]                  # see if episode has finished\n",
    "                coord = agent.env_info.vector_observations[0][-3:]\n",
    "\n",
    "                vel = agent.get_vel(prev_coord, coord, action_transformed)\n",
    "\n",
    "                sys.stdout.write('\\r episode:{}, step: {}, position:({:.2f} {:.2f} {:.2f}), score:{:.2f}, action:{}'.format(agent.name, ep_step,\n",
    "                    agent.env_info.vector_observations[0][-3:][0], agent.env_info.vector_observations[0][-3:][1],\n",
    "                    agent.env_info.vector_observations[0][-3:][2], ep_reward, str(action_idx)))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                positions.append([agent.env_info.vector_observations[0][-3:][0], \n",
    "                                 agent.env_info.vector_observations[0][-3:][1], \n",
    "                                 agent.env_info.vector_observations[0][-3:][2]])\n",
    "\n",
    "                ep_reward += reward\n",
    "                ep_step += 1\n",
    "\n",
    "                break\n",
    "                \n",
    "    prob_np = sess.run(prob, feed_dict={images: batch_img})\n",
    "    # print('prob_np:', prob_np)\n",
    "    print('prob_np:', prob_np.shape)\n",
    "    \n",
    "    # cost_np, gb_grad_value, target_conv_layer_value, target_conv_layer_grad_value = sess.run([cost, gb_grad, target_conv_layer, target_conv_layer_grad], feed_dict={images: batch_img, labels: prob_np})\n",
    "    net_np, y_c_np, gb_grad_value, target_conv_layer_value, target_conv_layer_grad_value = sess.run([net, y_c, gb_grad, target_conv_layer, target_conv_layer_grad], feed_dict={images: batch_img, labels: batch_label})\n",
    "    \n",
    "#     print(\"net_np:\", net_np)\n",
    "#     print(\"y_c_np:\", y_c_np)\n",
    "#     print(\"gb_grad_value:\", gb_grad_value)\n",
    "#     print(\"target_conv_layer_value:\", target_conv_layer_value)\n",
    "#     print(\"target_conv_layer_grad_value:\", target_conv_layer_grad_value)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # print('See visualization of below category')\n",
    "        # utils.print_prob(batch_label[i], './synset.txt')\n",
    "        utils.print_prob(prob_np[i], './synset.txt')\n",
    "        # print('gb_grad_value[i]:', gb_grad_value[i])\n",
    "        # print('gb_grad_value[i] shape:', gb_grad_value[i].shape)\n",
    "        utils.visualize(batch_img[i], target_conv_layer_value[i], target_conv_layer_grad_value[i], gb_grad_value[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-93cb43aa9879>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresnet_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mslim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nets'"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from nets import resnet_v1\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "import utils\n",
    "\n",
    "# Create mini-batch for demo\n",
    "\n",
    "img1 = utils.load_image(\"./demo.png\", normalize=False)\n",
    "img2 = utils.load_image(\"./shihtzu_mypuppy.jpg\", normalize=False)\n",
    "img3 = utils.load_image(\"./tiger.jpg\", normalize=False)\n",
    "\n",
    "batch1_img = img1.reshape((1, 224, 224, 3))\n",
    "batch1_label = np.array([1 if i == 242 else 0 for i in range(1000)])  # 1-hot result for Boxer\n",
    "batch1_label = batch1_label.reshape(1, -1)\n",
    "\n",
    "batch2_img = img2.reshape((1, 224, 224, 3))\n",
    "batch2_label = np.array([1 if i == 155 else 0 for i in range(1000)])  # 1-hot result for Shih-Tzu\n",
    "batch2_label = batch2_label.reshape(1, -1)\n",
    "\n",
    "batch3_img = img3.reshape((1, 224, 224, 3))\n",
    "batch3_label = np.array([1 if i == 292 else 0 for i in range(1000)])  # 1-hot result for tiger\n",
    "batch3_label = batch3_label.reshape(1, -1)\n",
    "\n",
    "batch_img = np.concatenate((batch1_img, batch2_img, batch3_img), 0)\n",
    "batch_label = np.concatenate((batch1_label, batch2_label, batch3_label), 0)\n",
    "batch_size = 3\n",
    "\n",
    "# batch_img = np.concatenate((batch1_img), 0)\n",
    "# batch_label = np.concatenate((batch1_label), 0)\n",
    "# batch_size = 1\n",
    "# batch_img = np.expand_dims(batch_img, 0)\n",
    "# batch_label = batch_label.reshape(batch_size, -1)\n",
    "\n",
    "# Create tensorflow graph for evaluation\n",
    "eval_graph = tf.Graph()\n",
    "with eval_graph.as_default():\n",
    "    with eval_graph.gradient_override_map({'Relu': 'GuidedRelu'}):\n",
    "        images = tf.placeholder(\"float\", [batch_size, 224, 224, 3])\n",
    "        labels = tf.placeholder(tf.float32, [batch_size, 1000])\n",
    "        \n",
    "        preprocessed_images = utils.resnet_preprocess(images)\n",
    "        \n",
    "        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "            with slim.arg_scope([slim.batch_norm], is_training=False):\n",
    "                # is_training=False means batch-norm is not in training mode. Fixing batch norm layer.\n",
    "                # net is logit for resnet_v1. See is_training messing up issue: https://github.com/tensorflow/tensorflow/issues/4887\n",
    "                net, end_points = resnet_v1.resnet_v1_50(preprocessed_images, 1000)\n",
    "        \n",
    "        prob = end_points['predictions'] # after softmax\n",
    "        print('prob:', prob)\n",
    "        \n",
    "        print('lables:', labels)\n",
    "        cost = (-1) * tf.reduce_sum(tf.multiply(labels, tf.log(prob)), axis=1)\n",
    "        print('cost:', cost)\n",
    "\n",
    "        # Get last convolutional layer gradient for generating gradCAM visualization\n",
    "        # print('endpoints:', end_points.keys())\n",
    "        target_conv_layer = end_points['resnet_v1_50/block4/unit_2/bottleneck_v1']\n",
    "        # target_conv_layer = end_points['resnet_v1_50/block3/unit_5/bottleneck_v1']\n",
    "        \n",
    "        # gradient for partial linearization. We only care about target visualization class. \n",
    "        y_c = tf.reduce_sum(tf.multiply(net, labels), axis=1)\n",
    "        print('y_c:', y_c)\n",
    "        target_conv_layer_grad = tf.gradients(y_c, target_conv_layer)[0]\n",
    "        print('target_conv_layer_grad:', target_conv_layer_grad)\n",
    "\n",
    "        # Guided backpropagtion back to input layer\n",
    "        gb_grad = tf.gradients(cost, images)[0]\n",
    "        print('gb_grad:', gb_grad)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Load resnet v1 weights\n",
    "        \n",
    "        # latest_checkpoint = tf.train.latest_checkpoint(\"model/resnet_v1_50.ckpt\")\n",
    "        latest_checkpoint = \"model/resnet_v1_50.ckpt\"\n",
    "        ## Optimistic restore.\n",
    "        reader = tf.train.NewCheckpointReader(latest_checkpoint)\n",
    "        saved_shapes = reader.get_variable_to_shape_map()\n",
    "        variables_to_restore = tf.global_variables()\n",
    "        for var in variables_to_restore:\n",
    "          if not var.name.split(':')[0] in saved_shapes:\n",
    "            print(\"WARNING. Saved weight not exists in checkpoint. Init var:\", var.name)\n",
    "          else:\n",
    "            # print(\"Load saved weight:\", var.name)\n",
    "            pass\n",
    "\n",
    "        var_names = sorted([(var.name, var.name.split(':')[0]) for var in variables_to_restore\n",
    "                if var.name.split(':')[0] in saved_shapes])\n",
    "        restore_vars = []\n",
    "        with tf.variable_scope('', reuse=True):\n",
    "            for var_name, saved_var_name in var_names:\n",
    "                try:\n",
    "                    curr_var = tf.get_variable(saved_var_name)\n",
    "                    var_shape = curr_var.get_shape().as_list()\n",
    "                    if var_shape == saved_shapes[saved_var_name]:\n",
    "                        # print(\"restore var:\", saved_var_name)\n",
    "                        restore_vars.append(curr_var)\n",
    "                    else:\n",
    "                        print(\"cannot restore var:\", saved_var_name)\n",
    "                except ValueError:\n",
    "                    print(\"Ignore due to ValueError on getting var:\", saved_var_name) \n",
    "        saver = tf.train.Saver(restore_vars)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Run tensorflow \n",
    "\n",
    "with tf.Session(graph=eval_graph) as sess:    \n",
    "    sess.run(init)\n",
    "    saver.restore(sess, latest_checkpoint)    \n",
    "    \n",
    "    prob_np = sess.run(prob, feed_dict={images: batch_img})\n",
    "    # print('prob_np:', prob_np)\n",
    "    print('prob_np:', prob_np.shape)\n",
    "    \n",
    "    # cost_np, gb_grad_value, target_conv_layer_value, target_conv_layer_grad_value = sess.run([cost, gb_grad, target_conv_layer, target_conv_layer_grad], feed_dict={images: batch_img, labels: prob_np})\n",
    "    net_np, y_c_np, gb_grad_value, target_conv_layer_value, target_conv_layer_grad_value = sess.run([net, y_c, gb_grad, target_conv_layer, target_conv_layer_grad], feed_dict={images: batch_img, labels: batch_label})\n",
    "    \n",
    "#     print(\"net_np:\", net_np)\n",
    "#     print(\"y_c_np:\", y_c_np)\n",
    "#     print(\"gb_grad_value:\", gb_grad_value)\n",
    "#     print(\"target_conv_layer_value:\", target_conv_layer_value)\n",
    "#     print(\"target_conv_layer_grad_value:\", target_conv_layer_grad_value)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # print('See visualization of below category')\n",
    "        # utils.print_prob(batch_label[i], './synset.txt')\n",
    "        utils.print_prob(prob_np[i], './synset.txt')\n",
    "        # print('gb_grad_value[i]:', gb_grad_value[i])\n",
    "        # print('gb_grad_value[i] shape:', gb_grad_value[i].shape)\n",
    "        utils.visualize(batch_img[i], target_conv_layer_value[i], target_conv_layer_grad_value[i], gb_grad_value[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-agents-tf",
   "language": "python",
   "name": "ml-agents-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
